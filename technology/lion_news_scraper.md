#!/usr/bin/env python3

# -*- coding: utf-8 -*-

# ==============================================================================

# ğŸ¦ lion_scraper.py (v5.3.0 â€“ UTC-safe + Reach & Recentness)

# - Proper robots.txt parsing (no false blocks)

# - Real User-Agent + Accept-Language

# - Retries/backoff for 403/429/timeouts

# - Selector fallbacks when primary yields 0

# - Date extractor: ancestor/sibling scan + relative times (â€œ4 days agoâ€)

# - Timezone-safe comparisons (UTC everywhere)

# ==============================================================================

  

import asyncio

import csv

import json

import logging

import re

import sys

import time

from datetime import datetime, timedelta, timezone

from pathlib import Path

from urllib.parse import urljoin, urlparse

  

from bs4 import BeautifulSoup

from dateutil import parser

from aiohttp_client_cache.session import CachedSession

from aiohttp_client_cache.backends.sqlite import SQLiteBackend

  

# ==============================================================================

# CONFIG

# ==============================================================================

USER_AGENT = (

Â  Â  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "

Â  Â  "AppleWebKit/537.36 (KHTML, like Gecko) "

Â  Â  "Chrome/124.0 Safari/537.36 LionScraper/5.3.0"

)

HEADERS = {

Â  Â  "User-Agent": USER_AGENT,

Â  Â  "Accept-Language": "en-US,en;q=0.9",

Â  Â  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",

}

  

MAX_RETRIES = 2

REQUEST_TIMEOUT = 15

CUTOFF_DAYS = 7

  

# ==============================================================================

# TIME HELPERS (UTC-normalization)

# ==============================================================================

def _to_utc(dt: datetime) -> datetime:

Â  Â  """Return a timezone-aware UTC datetime."""

Â  Â  if dt is None:

Â  Â  Â  Â  return None

Â  Â  return dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt.astimezone(timezone.utc)

  

def _now_utc() -> datetime:

Â  Â  return datetime.now(timezone.utc)

  

# ==============================================================================

# CONFIGURATION HANDLER

# ==============================================================================

class ScraperConfig:

Â  Â  def __init__(self):

Â  Â  Â  Â  self.script_dir = Path(__file__).parent.resolve()

Â  Â  Â  Â  self._load_paths()

Â  Â  Â  Â  self._validate_config()

  

Â  Â  def _load_paths(self):

Â  Â  Â  Â  self.config_path = self.script_dir / "config.json"

Â  Â  Â  Â  self.output_csv = self.script_dir / "lion_scraper_output.csv"

Â  Â  Â  Â  self.log_txt = self.script_dir / "lion_scraper_log.txt"

Â  Â  Â  Â  self.debug_csv = self.script_dir / "rejected_titles.csv"

Â  Â  Â  Â  self.cache_db = self.script_dir / "lion_cache.sqlite"

  

Â  Â  def _validate_config(self):

Â  Â  Â  Â  if not self.config_path.exists():

Â  Â  Â  Â  Â  Â  raise FileNotFoundError(f"âŒ Config not found: {self.config_path}")

Â  Â  Â  Â  with self.config_path.open("r", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  self.config = json.load(f)

Â  Â  Â  Â  self.sources = self.config.get("sources", [])

Â  Â  Â  Â  if not self.sources:

Â  Â  Â  Â  Â  Â  raise ValueError("âŒ No sources configured")

  

# ==============================================================================

# VALIDATION ENGINE

# ==============================================================================

class ArticleValidator:

Â  Â  @staticmethod

Â  Â  def is_valid_article(title: str, href: str, seen: set) -> bool:

Â  Â  Â  Â  title = (title or "").strip()

Â  Â  Â  Â  return all([

Â  Â  Â  Â  Â  Â  href,

Â  Â  Â  Â  Â  Â  len(title) >= 5,

Â  Â  Â  Â  Â  Â  title.lower() not in {"menu", "navigation", "read more", "sign up"},

Â  Â  Â  Â  Â  Â  title not in seen,

Â  Â  Â  Â  Â  Â  not re.match(r"^[\W\d_]+$", title),

Â  Â  Â  Â  ])

  

# ==============================================================================

# DATE PROCESSING

# ==============================================================================

class DateExtractor:

Â  Â  MONTHS_PATTERN = r"(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[A-Z]*"

Â  Â  DATE_PATTERNS = [

Â  Â  Â  Â  rf"\b{MONTHS_PATTERN}\s*\d{{1,2}},?\s*\d{{2,4}}\b",

Â  Â  Â  Â  rf"\b\d{{1,2}}\s*{MONTHS_PATTERN},?\s*\d{{2,4}}\b",

Â  Â  Â  Â  r"\b\d{4}-\d{1,2}-\d{1,2}\b",

Â  Â  Â  Â  r"\b\d{1,2}/\d{1,2}/\d{2,4}\b",

Â  Â  Â  Â  r"\b\d{1,2}\.\d{1,2}\.\d{2,4}\b",

Â  Â  ]

Â  Â  RELATIVE_RE = re.compile(r"\b(\d+)\s+(minute|hour|day|week|month|year)s?\s+ago\b", re.I)

  

Â  Â  @classmethod

Â  Â  def extract_date(cls, element):

Â  Â  Â  Â  # element + up to 3 ancestors

Â  Â  Â  Â  node = element

Â  Â  Â  Â  for _ in range(4):

Â  Â  Â  Â  Â  Â  if not node:

Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  dt = cls._check_structured(node) or cls._check_text(node)

Â  Â  Â  Â  Â  Â  if dt:

Â  Â  Â  Â  Â  Â  Â  Â  return _to_utc(dt)

Â  Â  Â  Â  Â  Â  node = getattr(node, "parent", None)

  

Â  Â  Â  Â  # siblings of original

Â  Â  Â  Â  if element and element.parent:

Â  Â  Â  Â  Â  Â  for s in element.parent.find_all(recursive=False):

Â  Â  Â  Â  Â  Â  Â  Â  if s is element:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  dt = cls._check_structured(s) or cls._check_text(s)

Â  Â  Â  Â  Â  Â  Â  Â  if dt:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return _to_utc(dt)

Â  Â  Â  Â  return None

  

Â  Â  @classmethod

Â  Â  def _relative_to_datetime(cls, text: str):

Â  Â  Â  Â  m = cls.RELATIVE_RE.search(text or "")

Â  Â  Â  Â  if not m:

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  n, unit = int(m.group(1)), m.group(2).lower()

Â  Â  Â  Â  minutes = {

Â  Â  Â  Â  Â  Â  "minute": 1, "hour": 60, "day": 24 * 60, "week": 7 * 24 * 60,

Â  Â  Â  Â  Â  Â  "month": 30 * 24 * 60, "year": 365 * 24 * 60,

Â  Â  Â  Â  }[unit]

Â  Â  Â  Â  return _now_utc() - timedelta(minutes=n * minutes)

  

Â  Â  @classmethod

Â  Â  def _check_structured(cls, element):

Â  Â  Â  Â  # <time> and timestamp-ish elements

Â  Â  Â  Â  for time_tag in element.select("time, [data-testid*='timestamp'], [class*='timestamp']"):

Â  Â  Â  Â  Â  Â  for attr in ("datetime", "data-datetime", "data-seconds", "data-time"):

Â  Â  Â  Â  Â  Â  Â  Â  val = time_tag.get(attr)

Â  Â  Â  Â  Â  Â  Â  Â  if val:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if attr == "data-seconds" and str(val).isdigit():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return datetime.fromtimestamp(int(val), tz=timezone.utc)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return parser.parse(str(val), fuzzy=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  Â  Â  # visible text inside time tag

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  txt = time_tag.get_text(strip=True)

Â  Â  Â  Â  Â  Â  Â  Â  if txt:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rel = cls._relative_to_datetime(txt)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if rel:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return rel

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return parser.parse(txt, fuzzy=True)

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  pass

  

Â  Â  Â  Â  # document-level meta

Â  Â  Â  Â  soup = element.find_parent("html")

Â  Â  Â  Â  if soup:

Â  Â  Â  Â  Â  Â  for meta in soup.select(

Â  Â  Â  Â  Â  Â  Â  Â  'meta[property="article:published_time"],'

Â  Â  Â  Â  Â  Â  Â  Â  'meta[property="article:modified_time"],'

Â  Â  Â  Â  Â  Â  Â  Â  'meta[name="OriginalPublicationDate"],'

Â  Â  Â  Â  Â  Â  Â  Â  'meta[name*="pubdate"], meta[property*="pubdate"],'

Â  Â  Â  Â  Â  Â  Â  Â  'meta[name*="date"], meta[property*="date"]'

Â  Â  Â  Â  Â  Â  ):

Â  Â  Â  Â  Â  Â  Â  Â  content = meta.get("content")

Â  Â  Â  Â  Â  Â  Â  Â  if content:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return parser.parse(str(content), fuzzy=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  # common date classes

Â  Â  Â  Â  for date_elem in element.select('*[class*="date"], *[class*="time"], *[class*="posted"]'):

Â  Â  Â  Â  Â  Â  txt = date_elem.get_text(" ", strip=True)

Â  Â  Â  Â  Â  Â  if not txt:

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  rel = cls._relative_to_datetime(txt)

Â  Â  Â  Â  Â  Â  if rel:

Â  Â  Â  Â  Â  Â  Â  Â  return rel

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  return parser.parse(txt, fuzzy=True)

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  return None

  

Â  Â  @classmethod

Â  Â  def _check_text(cls, element):

Â  Â  Â  Â  text = element.get_text(" ", strip=True)

  

Â  Â  Â  Â  # relative first

Â  Â  Â  Â  rel = cls._relative_to_datetime(text)

Â  Â  Â  Â  if rel:

Â  Â  Â  Â  Â  Â  return rel

  

Â  Â  Â  Â  # absolute formats

Â  Â  Â  Â  for pattern in cls.DATE_PATTERNS:

Â  Â  Â  Â  Â  Â  m = re.search(pattern, text, re.IGNORECASE)

Â  Â  Â  Â  Â  Â  if not m:

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  dt = parser.parse(m.group(), fuzzy=True)

Â  Â  Â  Â  Â  Â  Â  Â  if dt.year in (datetime.utcnow().year, datetime.utcnow().year - 1):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return dt

Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  low = text.lower()

Â  Â  Â  Â  if "today" in low or "just now" in low:

Â  Â  Â  Â  Â  Â  return _now_utc()

Â  Â  Â  Â  if "yesterday" in low:

Â  Â  Â  Â  Â  Â  return _now_utc() - timedelta(days=1)

Â  Â  Â  Â  return None

  

# ==============================================================================

# SCRAPER ENGINE

# ==============================================================================

class ScraperEngine:

Â  Â  def __init__(self, config):

Â  Â  Â  Â  self.config = config

Â  Â  Â  Â  self._setup_logging()

  

Â  Â  def _setup_logging(self):

Â  Â  Â  Â  logging.basicConfig(

Â  Â  Â  Â  Â  Â  filename=self.config.log_txt,

Â  Â  Â  Â  Â  Â  level=logging.INFO,

Â  Â  Â  Â  Â  Â  format="%(asctime)s | %(levelname)s | %(message)s",

Â  Â  Â  Â  Â  Â  force=True,

Â  Â  Â  Â  )

Â  Â  Â  Â  self.logger = logging.getLogger()

  

Â  Â  async def run(self):

Â  Â  Â  Â  start_time = time.time()

Â  Â  Â  Â  results = await self._scrape_all_sources()

Â  Â  Â  Â  self._save_results(results)

Â  Â  Â  Â  self._log_summary(results, start_time)

  

Â  Â  async def _scrape_all_sources(self):

Â  Â  Â  Â  async with CachedSession(

Â  Â  Â  Â  Â  Â  cache=SQLiteBackend(str(self.config.cache_db)),

Â  Â  Â  Â  Â  Â  headers=HEADERS,

Â  Â  Â  Â  ) as session:

Â  Â  Â  Â  Â  Â  tasks = [self._scrape_source(session, src) for src in self.config.sources]

Â  Â  Â  Â  Â  Â  return await asyncio.gather(*tasks)

  

Â  Â  # ----------- HTTP helpers -------------------------------------------------

Â  Â  async def _fetch_text(self, session, url, timeout=REQUEST_TIMEOUT):

Â  Â  Â  Â  last_exc = None

Â  Â  Â  Â  for attempt in range(MAX_RETRIES + 1):

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  async with session.get(url, timeout=timeout) as resp:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if resp.status in (403, 429) and attempt < MAX_RETRIES:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await asyncio.sleep(1.0 + attempt)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  resp.raise_for_status()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return await resp.text()

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  last_exc = e

Â  Â  Â  Â  Â  Â  Â  Â  if attempt < MAX_RETRIES:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await asyncio.sleep(0.6 * (attempt + 1))

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.logger.error(f"âŒ fetch failed [{url}]: {e}")

Â  Â  Â  Â  raise last_exc

  

Â  Â  async def _robots_allows(self, session, base_url: str) -> bool:

Â  Â  Â  Â  """

Â  Â  Â  Â  Parse robots.txt; allow if:

Â  Â  Â  Â  Â  - no robots.txt, or cannot parse, or

Â  Â  Â  Â  Â  - '*' does NOT explicitly disallow '/'.

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  parsed = urlparse(base_url)

Â  Â  Â  Â  Â  Â  robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

Â  Â  Â  Â  Â  Â  text = await self._fetch_text(session, robots_url, timeout=6)

Â  Â  Â  Â  Â  Â  rules = []

Â  Â  Â  Â  Â  Â  agent = None

Â  Â  Â  Â  Â  Â  for line in text.splitlines():

Â  Â  Â  Â  Â  Â  Â  Â  line = line.strip()

Â  Â  Â  Â  Â  Â  Â  Â  if not line or line.startswith("#"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  if line.lower().startswith("user-agent:"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  agent = line.split(":", 1)[1].strip()

Â  Â  Â  Â  Â  Â  Â  Â  elif line.lower().startswith("disallow:"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  path = line.split(":", 1)[1].strip() or "/"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rules.append((agent, path))

Â  Â  Â  Â  Â  Â  # If any rule for '*' (or our UA) disallows '/', block; else allow.

Â  Â  Â  Â  Â  Â  for ua, path in rules:

Â  Â  Â  Â  Â  Â  Â  Â  if ua in ("*", USER_AGENT) and path == "/":

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.logger.warning(f"ğŸš« robots disallow all at {base_url}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  self.logger.info(f"âš ï¸ robots not enforced for {base_url}: {e}")

Â  Â  Â  Â  return True

  

Â  Â  # ----------- Source scrape ------------------------------------------------

Â  Â  async def _scrape_source(self, session, source):

Â  Â  Â  Â  base = source["url"].rstrip("/")

Â  Â  Â  Â  if not await self._robots_allows(session, base):

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  html = await self._fetch_text(session, base, timeout=REQUEST_TIMEOUT)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  self.logger.error(f"âŒ {source['name']} fetch error: {e}")

Â  Â  Â  Â  Â  Â  return []

  

Â  Â  Â  Â  return self._process_html(html, source)

  

Â  Â  def _process_html(self, html, source):

Â  Â  Â  Â  soup = BeautifulSoup(html, "html.parser")

  

Â  Â  Â  Â  primary = source.get("article_selector", "")

Â  Â  Â  Â  fallbacks = [

Â  Â  Â  Â  Â  Â  "article a",

Â  Â  Â  Â  Â  Â  "h2 a, h3 a",

Â  Â  Â  Â  Â  Â  "a[href*='/news/'], a[href*='/story'], a[href*='/article']",

Â  Â  Â  Â  ]

Â  Â  Â  Â  selectors = [s for s in [primary] if s] + fallbacks

  

Â  Â  Â  Â  elements = []

Â  Â  Â  Â  used_selector = "NONE"

Â  Â  Â  Â  for sel in selectors:

Â  Â  Â  Â  Â  Â  elements = soup.select(sel)

Â  Â  Â  Â  Â  Â  if elements:

Â  Â  Â  Â  Â  Â  Â  Â  used_selector = sel

Â  Â  Â  Â  Â  Â  Â  Â  break

  

Â  Â  Â  Â  self.logger.info(f"ğŸ” {source['name']}: {len(elements)} elements via selector '{used_selector}'")

  

Â  Â  Â  Â  seen = set()

Â  Â  Â  Â  results, rejected = [], []

Â  Â  Â  Â  cutoff_date = _now_utc() - timedelta(days=CUTOFF_DAYS)

  

Â  Â  Â  Â  for element in elements[:200]:

Â  Â  Â  Â  Â  Â  a_tag = self._extract_link(element)

Â  Â  Â  Â  Â  Â  if not a_tag:

Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  Â  Â  title, href = self._extract_title_and_href(a_tag)

Â  Â  Â  Â  Â  Â  if not ArticleValidator.is_valid_article(title, href, seen):

Â  Â  Â  Â  Â  Â  Â  Â  rejected.append(self._rej(source, "invalid", title, href))

Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  Â  Â  # richer container for dates

Â  Â  Â  Â  Â  Â  container = a_tag.parent if a_tag.name == "a" else a_tag

Â  Â  Â  Â  Â  Â  if container and container.parent and container.parent.name not in ("html", "[document]"):

Â  Â  Â  Â  Â  Â  Â  Â  container = container.parent

Â  Â  Â  Â  Â  Â  pub_dt = DateExtractor.extract_date(container or element)

  

Â  Â  Â  Â  Â  Â  if not pub_dt:

Â  Â  Â  Â  Â  Â  Â  Â  results.append(self._res(source, title, href, None))

Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  Â  Â  if pub_dt < cutoff_date:

Â  Â  Â  Â  Â  Â  Â  Â  rejected.append(self._rej(source, f"too_old ({pub_dt:%Y-%m-%d})", title, href))

Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  Â  Â  seen.add(title)

Â  Â  Â  Â  Â  Â  results.append(self._res(source, title, href, pub_dt))

  

Â  Â  Â  Â  self._log_results(source, results, rejected)

Â  Â  Â  Â  return results

  

Â  Â  # ----------- Helpers ------------------------------------------------------

Â  Â  def _extract_link(self, element):

Â  Â  Â  Â  return element if element.name == "a" else element.find("a")

  

Â  Â  def _extract_title_and_href(self, a_tag):

Â  Â  Â  Â  return a_tag.get_text(strip=True), a_tag.get("href", "")

  

Â  Â  def _rej(self, source, reason, title, href):

Â  Â  Â  Â  return {"source": source["name"], "reason": reason, "title": title, "url": href}

  

Â  Â  def _res(self, source, title, href, pub_dt):

Â  Â  Â  Â  full_url = urljoin(source["url"], str(href))

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "source": source["name"],

Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  "url": f'=HYPERLINK("{full_url}", "Go!")',

Â  Â  Â  Â  Â  Â  "date": pub_dt.strftime("%Y-%m-%d") if pub_dt else "undated",

Â  Â  Â  Â  Â  Â  "timestamp": time.time(),

Â  Â  Â  Â  }

  

Â  Â  def _log_results(self, source, results, rejected):

Â  Â  Â  Â  if results:

Â  Â  Â  Â  Â  Â  dated = [r for r in results if r["date"] != "undated"]

Â  Â  Â  Â  Â  Â  info = f" | Dated: {len(dated)}" if dated else ""

Â  Â  Â  Â  Â  Â  self.logger.info(f"âœ… {source['name']}: {len(results)} articles{info}")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  self.logger.warning(f"âš ï¸ {source['name']}: No valid articles")

Â  Â  Â  Â  if rejected:

Â  Â  Â  Â  Â  Â  with open(self.config.debug_csv, "a", newline="", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  Â  Â  writer = csv.DictWriter(f, fieldnames=["source", "reason", "title", "url"])

Â  Â  Â  Â  Â  Â  Â  Â  if f.tell() == 0:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  writer.writeheader()

Â  Â  Â  Â  Â  Â  Â  Â  writer.writerows(rejected)

  

Â  Â  def _save_results(self, results):

Â  Â  Â  Â  flat = [item for sublist in results for item in sublist]

Â  Â  Â  Â  if not flat:

Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  with open(self.config.output_csv, "w", newline="", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  writer = csv.DictWriter(f, fieldnames=["source", "title", "url", "date", "timestamp"])

Â  Â  Â  Â  Â  Â  writer.writeheader()

Â  Â  Â  Â  Â  Â  writer.writerows(flat)

  

Â  Â  def _log_summary(self, results, start_time):

Â  Â  Â  Â  flat = [item for sublist in results for item in sublist]

Â  Â  Â  Â  dated = [r for r in flat if r["date"] != "undated"]

  

Â  Â  Â  Â  if dated:

Â  Â  Â  Â  Â  Â  # parse dates as naive YYYY-MM-DD for summary only

Â  Â  Â  Â  Â  Â  dates = [datetime.strptime(r["date"], "%Y-%m-%d") for r in dated]

Â  Â  Â  Â  Â  Â  date_range = f"{min(dates):%Y-%m-%d} to {max(dates):%Y-%m-%d}"

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  date_range = "N/A"

  

Â  Â  Â  Â  summary = (

Â  Â  Â  Â  Â  Â  f"\nğŸ§¾ Summary:\n"

Â  Â  Â  Â  Â  Â  f"âœ… Total articles: {len(flat)}\n"

Â  Â  Â  Â  Â  Â  f"ğŸ“… Dated articles: {len(dated)}\n"

Â  Â  Â  Â  Â  Â  f"ğŸ“… Date range: {date_range}\n"

Â  Â  Â  Â  Â  Â  f"ğŸ“„ CSV saved to: {self.config.output_csv}\n"

Â  Â  Â  Â  Â  Â  f"ğŸªµ Logs: {self.config.log_txt}\n"

Â  Â  Â  Â  Â  Â  f"â³ Duration: {time.time() - start_time:.2f}s\n"

Â  Â  Â  Â  Â  Â  f"ğŸ§ƒ Rejections: {self.config.debug_csv}\n"

Â  Â  Â  Â  )

Â  Â  Â  Â  print(summary)

Â  Â  Â  Â  self.logger.info(summary)

  

# ==============================================================================

# MAIN

# ==============================================================================

def main():

Â  Â  try:

Â  Â  Â  Â  cfg = ScraperConfig()

Â  Â  Â  Â  engine = ScraperEngine(cfg)

Â  Â  Â  Â  asyncio.run(engine.run())

Â  Â  except Exception as e:

Â  Â  Â  Â  logging.critical(f"ğŸ”¥ Catastrophic failure: {e}")

Â  Â  Â  Â  print(f"ğŸ’¥ Critical error: {e}")

Â  Â  Â  Â  sys.exit(1)

  

if __name__ == "__main__":

Â  Â  main()