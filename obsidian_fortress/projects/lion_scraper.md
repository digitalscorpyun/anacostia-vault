---
id: '20250511112837'
title: Lion Scraper
category: ''
style: ScorpyunStyle
path: ''
created: '2025-05-11'
updated: '2025-05-11'
status: in_progress
priority: normal
summary: ''
longform_summary: ''
tags: []
cssclasses:
- tyrian-purple
- sacred-tech
synapses: []
key_themes: []
bias_analysis: ''
grok_ctx_reflection: ''
quotes: []
adinkra: []
linked_notes: []
---

# lion_scraper.py (v4.3)

  

import aiohttp

import asyncio

import csv

import json

import logging

from bs4 import BeautifulSoup

from pathlib import Path

  

# Load config relative to script

script_dir = Path(__file__).parent

config_path = script_dir / "config_patched.json"

  

with open(config_path, "r", encoding="utf-8") as f:

Â  Â  config = json.load(f)

  

sources = config["sources"]

output_csv = config["output_csv"]

log_txt = config["log_txt"]

debug_csv = script_dir / "rejected_titles.csv"

  

# Set up logging

logging.basicConfig(

Â  Â  filename=log_txt,

Â  Â  level=logging.INFO,

Â  Â  format="%(asctime)s | %(levelname)s | %(message)s"

)

logger = logging.getLogger()

  
  

# Helper function to scrape each source

async def fetch(session, source):

Â  Â  try:

Â  Â  Â  Â  async with session.get(source["url"], timeout=10) as response:

Â  Â  Â  Â  Â  Â  html = await response.text()

Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(html, "html.parser")

Â  Â  Â  Â  Â  Â  articles = soup.select(source["article_selector"])

  

Â  Â  Â  Â  Â  Â  seen = set()

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â  rejected = []

  

Â  Â  Â  Â  Â  Â  for a in articles:

Â  Â  Â  Â  Â  Â  Â  Â  title = a.get_text(strip=True)

Â  Â  Â  Â  Â  Â  Â  Â  href = a.get("href")

  

Â  Â  Â  Â  Â  Â  Â  Â  if not href or len(title) < 5 or title.lower() in {"navigation", "menu"} or title in seen:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rejected.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "source": source["name"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "reason": "filtered",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "url": href

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  Â  Â  Â  Â  seen.add(title)

  

Â  Â  Â  Â  Â  Â  Â  Â  full_url = (

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  href

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if href.startswith("http")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else source["url"].rstrip("/") + "/" + href.lstrip("/")

Â  Â  Â  Â  Â  Â  Â  Â  )

  

Â  Â  Â  Â  Â  Â  Â  Â  results.append({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "source": source["name"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "title": title,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "url": f'=HYPERLINK("{full_url}", "Go!")'

Â  Â  Â  Â  Â  Â  Â  Â  })

  

Â  Â  Â  Â  Â  Â  Â  Â  if len(results) >= 20:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

  

Â  Â  Â  Â  Â  Â  if results:

Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… {source['name']}: {len(results)} articles scraped")

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"âš ï¸ {source['name']}: No valid articles (check selectors or filters)"

Â  Â  Â  Â  Â  Â  Â  Â  )

  

Â  Â  Â  Â  Â  Â  # Write rejected for debugging

Â  Â  Â  Â  Â  Â  if rejected:

Â  Â  Â  Â  Â  Â  Â  Â  with open(debug_csv, "a", newline="", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  writer = csv.DictWriter(f, fieldnames=["source", "reason", "title", "url"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if f.tell() == 0:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  writer.writeheader()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  writer.writerows(rejected)

  

Â  Â  Â  Â  Â  Â  return results

  

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.warning(f"âŒ {source['name']}: Failed - {e}")

Â  Â  Â  Â  return []

  
  

# Orchestrator

async def main():

Â  Â  all_results = []

  

Â  Â  async with aiohttp.ClientSession() as session:

Â  Â  Â  Â  tasks = [fetch(session, site) for site in sources]

Â  Â  Â  Â  results = await asyncio.gather(*tasks)

  

Â  Â  Â  Â  for r in results:

Â  Â  Â  Â  Â  Â  all_results.extend(r)

  

Â  Â  if all_results:

Â  Â  Â  Â  with open(output_csv, "w", newline="", encoding="utf-8") as f:

Â  Â  Â  Â  Â  Â  writer = csv.DictWriter(f, fieldnames=["source", "title", "url"])

Â  Â  Â  Â  Â  Â  writer.writeheader()

Â  Â  Â  Â  Â  Â  writer.writerows(all_results)

  

Â  Â  summary = (

Â  Â  Â  Â  f"âœ… Scraping complete. Articles scraped: {len(all_results)}\n"

Â  Â  Â  Â  f"ğŸ“„ CSV saved at: {output_csv}\n"

Â  Â  Â  Â  f"ğŸªµ Log file at: {log_txt}\n"

Â  Â  Â  Â  f"ğŸ” Rejected entries in: {debug_csv}"

Â  Â  )

  

Â  Â  print(summary)

  

Â  Â  with open(log_txt, "a", encoding="utf-8") as log_file:

Â  Â  Â  Â  log_file.write("\n" + summary + "\n")

  
  

if __name__ == "__main__":

Â  Â  asyncio.run(main())